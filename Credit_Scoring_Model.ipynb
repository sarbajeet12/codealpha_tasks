{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**CREDIT SCORING MODEL**"
      ],
      "metadata": {
        "id": "hjzuxqd9Vmwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports & Data Loading – Bring in Python libraries and load your dataset."
      ],
      "metadata": {
        "id": "peDCgDe_VwjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports & Data Loading\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import joblib"
      ],
      "metadata": {
        "id": "25iv4gqKWryO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset (replace with your path/filename)\n",
        "credit_data= pd.read_csv(r'/content/credit_scoring.csv')\n",
        "credit_data"
      ],
      "metadata": {
        "id": "h2sp_mVPWv6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Exploratory Data Analysis (EDA) – quick checks\n",
        "credit_data.head()"
      ],
      "metadata": {
        "id": "LUn7MfzcW83O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_data.isnull().sum()"
      ],
      "metadata": {
        "id": "a6RcqoZfYJQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(credit_data.head())\n",
        "print(credit_data.info())\n",
        "print(credit_data.isnull().sum())"
      ],
      "metadata": {
        "id": "Wvq5OTmoYPxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16209240"
      },
      "source": [
        "**Reasoning**:\n",
        "Examine the columns of the `credit_data` DataFrame to identify a column that indicates creditworthiness or a credit score. Since there is no explicit target variable, I will create one based on 'Credit Utilization Ratio' and 'Payment History'. A lower credit utilization ratio and higher payment history indicate better creditworthiness. I will define creditworthy as having a Credit Utilization Ratio less than 0.5 and Payment History greater than the median.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd6081dc"
      },
      "source": [
        "print(credit_data.columns)\n",
        "\n",
        "median_payment_history = credit_data['Payment History'].median()\n",
        "credit_data['Creditworthy'] = ((credit_data['Credit Utilization Ratio'] < 0.5) & (credit_data['Payment History'] > median_payment_history)).astype(int)\n",
        "print(credit_data[['Credit Utilization Ratio', 'Payment History', 'Creditworthy']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "866da9ae"
      },
      "source": [
        "## **Define features & target**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7abc5397"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the target column was not correctly specified. Based on the previous subtask, the target column is 'Creditworthy'. This code block will correctly set the target column and split the data into features (X) and target (y).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ff1a6f4"
      },
      "source": [
        "target_column = 'Creditworthy'\n",
        "X = credit_data.drop(columns=[target_column])\n",
        "y = credit_data[target_column]\n",
        "\n",
        "print(\"Features (X) shape:\", X.shape)\n",
        "print(\"Target (y) shape:\", y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79d7d5a0"
      },
      "source": [
        "## **Data preprocessing**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0802967"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify categorical and numerical columns, apply one-hot encoding to categorical columns, scale numerical columns, and combine them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7507da5c"
      },
      "source": [
        "categorical_cols = X.select_dtypes(include='object').columns\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_encoded[numerical_cols] = scaler.fit_transform(X_encoded[numerical_cols])\n",
        "\n",
        "display(X_encoded.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e54e97f8"
      },
      "source": [
        "## **Split data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "345f42a3"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the preprocessed data into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2781c9f4"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe2d0ee0"
      },
      "source": [
        "## **Model selection & training**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffb2ec16"
      },
      "source": [
        "**Reasoning**:\n",
        "Train a Logistic Regression model using the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "908612b4"
      },
      "source": [
        "log_reg_model = LogisticRegression(random_state=42)\n",
        "log_reg_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2cfb585"
      },
      "source": [
        "**Reasoning**:\n",
        "Train a Decision Tree Classifier model using the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "772ff774"
      },
      "source": [
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64c93706"
      },
      "source": [
        "**Reasoning**:\n",
        "Train a Random Forest Classifier model using the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f442338"
      },
      "source": [
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c30f3c2"
      },
      "source": [
        "## **Model evaluation**\n",
        "\n",
        "\n",
        "Evaluate the trained model using appropriate metrics (e.g., precision, recall, F1-score, ROC-AUC).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcbcf7fa"
      },
      "source": [
        "**Reasoning**:\n",
        "Make predictions and calculate evaluation metrics for each trained model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcd10390"
      },
      "source": [
        "# Make predictions\n",
        "y_pred_lr = log_reg_model.predict(X_test)\n",
        "y_proba_lr = log_reg_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "y_proba_dt = dt_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "metrics = {\n",
        "    'Logistic Regression': {\n",
        "        'Precision': precision_score(y_test, y_pred_lr),\n",
        "        'Recall': recall_score(y_test, y_pred_lr),\n",
        "        'F1-score': f1_score(y_test, y_pred_lr),\n",
        "        'ROC-AUC': roc_auc_score(y_test, y_proba_lr)\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'Precision': precision_score(y_test, y_pred_dt),\n",
        "        'Recall': recall_score(y_test, y_pred_dt),\n",
        "        'F1-score': f1_score(y_test, y_pred_dt),\n",
        "        'ROC-AUC': roc_auc_score(y_test, y_proba_dt)\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'Precision': precision_score(y_test, y_pred_rf),\n",
        "        'Recall': recall_score(y_test, y_pred_rf),\n",
        "        'F1-score': f1_score(y_test, y_pred_rf),\n",
        "        'ROC-AUC': roc_auc_score(y_test, y_proba_rf)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Print metrics\n",
        "for model, model_metrics in metrics.items():\n",
        "    print(f\"{model} Metrics:\")\n",
        "    for metric, value in model_metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Assume y_test and predicted probabilities y_proba_lr, y_proba_dt, y_proba_rf exist\n",
        "\n",
        "# Compute FPR and TPR for each model\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)\n",
        "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_proba_dt)\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
        "\n",
        "# Compute AUC values\n",
        "auc_lr = roc_auc_score(y_test, y_proba_lr)\n",
        "auc_dt = roc_auc_score(y_test, y_proba_dt)\n",
        "auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
        "\n",
        "# Plot all ROC curves on one figure\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.3f})')\n",
        "plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {auc_dt:.3f})')\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve Comparison')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dgDyAML-RAKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "# Plot the first model\n",
        "disp_lr = RocCurveDisplay.from_estimator(log_reg_model, X_test, y_test, name='Logistic Regression')\n",
        "ax = disp_lr.ax_\n",
        "\n",
        "# Overlay additional models\n",
        "RocCurveDisplay.from_estimator(dt_model, X_test, y_test, name='Decision Tree', ax=ax)\n",
        "RocCurveDisplay.from_estimator(rf_model, X_test, y_test, name='Random Forest', ax=ax)\n",
        "\n",
        "ax.set_title('ROC Curve Comparison')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6zpP22fORHo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "\n",
        "# --- 6. Bar Plot for Metrics ---\n",
        "# Create a DataFrame from the metrics dictionary\n",
        "df_results = pd.DataFrame(metrics).T\n",
        "df_results.plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Model Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yREJ796ZRK0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5975ded3"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A new binary target variable 'Creditworthy' was engineered based on 'Credit Utilization Ratio' and 'Payment History' because the original dataset lacked a target column for creditworthiness.\n",
        "*   Categorical features were successfully one-hot encoded, and numerical features were scaled using `StandardScaler`.\n",
        "*   The preprocessed data was split into training (80%) and testing (20%) sets.\n",
        "*   Three classification models (Logistic Regression, Decision Tree, and Random Forest) were trained on the training data.\n",
        "*   Model evaluation on the test set showed that the Decision Tree and Random Forest models achieved perfect scores (Precision, Recall, F1-score, ROC-AUC = 1.0000), while the Logistic Regression model also performed well but not perfectly.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The perfect scores achieved by the Decision Tree and Random Forest models suggest potential overfitting or that the engineered target variable is too simple and easily separable based on the chosen features. Further investigation into the data and the definition of 'Creditworthy' is needed.\n",
        "*   Consider using cross-validation during training to get a more robust estimate of model performance and explore more complex feature engineering or model architectures if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Y-DATA PRIOFILING**"
      ],
      "metadata": {
        "id": "B9SBCvZBQoYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ydata-profiling\n"
      ],
      "metadata": {
        "id": "Mn8Eez8fATF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "prof = ProfileReport(credit_data)\n",
        "prof.to_file(output_file='EDA.html')\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(prof.to_html()))"
      ],
      "metadata": {
        "id": "RCdzZim_HtBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i2WJVaJjH8TF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
